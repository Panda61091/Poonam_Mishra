{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMebbRGVtoDKx/aWv+GpINd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Panda61091/Poonam_Mishra/blob/master/1st_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARC0PuQxTErn"
      },
      "source": [
        "import logging\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from numpy import random\r\n",
        "import gensim\r\n",
        "import nltk\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import re\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboIdqKKTa3j",
        "outputId": "bcfba1ea-5054-4b17-a0c8-fbbb0e94c13c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIviaAEcTlMq"
      },
      "source": [
        "import pandas as pd\r\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeLGXjE3TlPs"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Master_Balancesheet.csv',encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBkmgyuuTlSu",
        "outputId": "a6728bdf-98d2-4dff-ade5-e3b3d882f4ce"
      },
      "source": [
        "df = df[pd.notnull(df['category'])]\r\n",
        "print(df.head(10))\r\n",
        "print(df['extracted_lineitems'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                           extracted_lineitems  ...             context\n",
            "0                                       assets  ...              assets\n",
            "1                               current assets  ...      current assets\n",
            "2                    cash and cash equivalents  ...      current assets\n",
            "3                  trade and other receivables  ...      current assets\n",
            "4                                  inventories  ...      current assets\n",
            "5              current portion of other assets  ...      current assets\n",
            "6                           non-current assets  ...  non current assets\n",
            "7  cash and cash equivalents - restricted cash  ...  non current assets\n",
            "8                   non-current vat receivable  ...  non current assets\n",
            "9                property, plant and equipment  ...  non current assets\n",
            "\n",
            "[10 rows x 5 columns]\n",
            "8793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnOzuZRWTlVk",
        "outputId": "05dbb6f8-5b9f-4595-a3f1-674be1211010"
      },
      "source": [
        "def print_plot(index):\r\n",
        "    example = df[df.index == index][['extracted_lineitems', 'category']].values[0]\r\n",
        "    if len(example) > 0:\r\n",
        "        print(example[0])\r\n",
        "        print('Category:', example[1])\r\n",
        "print_plot(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "goodwill\n",
            "Category: goodwill\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcM3ZmS6TlYc",
        "outputId": "f98d2ae6-0271-4714-9fba-2835d32b2146"
      },
      "source": [
        "print_plot(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accumulated other comprehensive loss\n",
            "Category: comprehensive inc. and other\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRpmUSX_TlbL",
        "outputId": "6f296e12-dfd3-40e6-8853-bf347e24607d"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJAofWqsUSQs",
        "outputId": "351265e9-c9cb-43fe-a9ba-1dba7041ff29"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\r\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\r\n",
        "STOPWORDS = set(stopwords.words('english'))\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    \"\"\"\r\n",
        "        text: a string\r\n",
        "        \r\n",
        "        return: modified initial string\r\n",
        "    \"\"\"\r\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\r\n",
        "    text = text.lower() # lowercase text\r\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\r\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\r\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\r\n",
        "    return text\r\n",
        "    \r\n",
        "df['extracted_lineitems'] = df['extracted_lineitems'].apply(clean_text)\r\n",
        "print_plot(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "goodwill\n",
            "Category: goodwill\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6ZLmdAbUSTy",
        "outputId": "b1b1a378-fbe2-401e-cf85-2ff04b73ec11"
      },
      "source": [
        "df['extracted_lineitems'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7lPmw6FUSWs"
      },
      "source": [
        "X = df.extracted_lineitems\r\n",
        "y = df.category\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9PMqTYrUSaD"
      },
      "source": [
        "##Word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQpskiNKTleH"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\r\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyuTqogFTlhi",
        "outputId": "53d35327-2bda-40a3-f091-ec5ff363a4d5"
      },
      "source": [
        "from itertools import islice\r\n",
        "list(islice(wv.vocab, 13030, 13050))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Memorial_Hospital',\n",
              " 'Seniors',\n",
              " 'memorandum',\n",
              " 'elephant',\n",
              " 'Trump',\n",
              " 'Census',\n",
              " 'pilgrims',\n",
              " 'De',\n",
              " 'Dogs',\n",
              " '###-####_ext',\n",
              " 'chaotic',\n",
              " 'forgive',\n",
              " 'scholar',\n",
              " 'Lottery',\n",
              " 'decreasing',\n",
              " 'Supervisor',\n",
              " 'fundamentally',\n",
              " 'Fitness',\n",
              " 'abundance',\n",
              " 'Hold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_KeDcNaTMBB"
      },
      "source": [
        "def word_averaging(wv, words):\r\n",
        "    all_words, mean = set(), []\r\n",
        "    \r\n",
        "    for word in words:\r\n",
        "        if isinstance(word, np.ndarray):\r\n",
        "            mean.append(word)\r\n",
        "        elif word in wv.vocab:\r\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\r\n",
        "            all_words.add(wv.vocab[word].index)\r\n",
        "\r\n",
        "    if not mean:\r\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\r\n",
        "        # FIXME: remove these examples in pre-processing\r\n",
        "        return np.zeros(wv.vector_size,)\r\n",
        "\r\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\r\n",
        "    return mean\r\n",
        "\r\n",
        "def  word_averaging_list(wv, text_list):\r\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIiYO7I3VVMX",
        "outputId": "e24834b3-e978-4547-c45c-2c37bcbe4b5e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLnPHKc7UpZF",
        "outputId": "394c055e-0b25-44a6-c518-db0912480b10"
      },
      "source": [
        "def w2v_tokenize_text(text):\r\n",
        "    tokens = []\r\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\r\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\r\n",
        "            if len(word) < 2:\r\n",
        "                continue\r\n",
        "            tokens.append(word)\r\n",
        "    return tokens\r\n",
        "    \r\n",
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\r\n",
        "\r\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['extracted_lineitems']), axis=1).values\r\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['extracted_lineitems']), axis=1).values\r\n",
        "\r\n",
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\r\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  \n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input ['goodwil']\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input ['goodwil']\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input []\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNk836O9VplL"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSySdSavUpcn",
        "outputId": "2f787fc4-acd3-4564-e558-2abbf7ac5cb1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "#logreg = LogisticRegression(n_jobs=1, C=1e10)\r\n",
        "logreg = LogisticRegression()\r\n",
        "logreg = logreg.fit(X_train_word_average, train['category'])\r\n",
        "y_pred = logreg.predict(X_test_word_average)\r\n",
        "print('accuracy %s' % accuracy_score(y_pred, test.category))\r\n",
        "#print(classification_report(test.category, y_pred))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.629500580720093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y69Q8DKVn7u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}